{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Problem Statement:\n",
    "\n",
    "__About Company__ <br>\n",
    "Dream Housing Finance company deals in all home loans. They have presence across all urban, semi urban and rural areas. Customer first apply for home loan after that company validates the customer eligibility for loan.\n",
    "\n",
    "__Problem__ <br>\n",
    "Company wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers. Here they have provided a partial data set.\n",
    "\n",
    "#### Dataset Description:\n",
    "\n",
    "| Variable | Description | \n",
    "|------|------|\n",
    "| Loan_ID | Unique Loan ID | \n",
    "| Gender | Male/ Female | \n",
    "| Married | Applicant married (Y/N) | \n",
    "| Dependents | Number of dependents | \n",
    "| Education | Applicant Education (Graduate/ Under Graduate) | \n",
    "| Self_Employed | Self employed (Y/N) | \n",
    "| ApplicantIncome | Applicant income | \n",
    "| CoapplicantIncome | Coapplicant income | \n",
    "| LoanAmount | Loan amount in thousands | \n",
    "| Loan_Amount_Term | Term of loan in months | \n",
    "| Credit_History | credit history meets guidelines | \n",
    "| Property_Area | Urban/ Semi Urban/ Rural | \n",
    "| Loan_Status | Loan approved (Y/N) | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Import dependencies and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #importing seaborn module \n",
    "import warnings\n",
    "import os, joblib, operator\n",
    "from sklearn.model_selection import train_test_split,KFold, cross_val_score, cross_val_predict,cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from minio import Minio\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from mlflow.tracking import MlflowClient\n",
    "import mlflow, urllib3\n",
    "from mlflow import pyfunc\n",
    "\n",
    "warnings.filterwarnings('ignore')  #this will ignore the warnings.it wont display warnings in notebook\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize']=[6,3]\n",
    "plt.rcParams['figure.dpi']=80\n",
    "\n",
    "os.environ[\"BUCKET\"] = \"loan-prediction\"\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"https://mip-bdcs-vm28.mip.storage.hpecorp.net:10021\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"admin\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"admin123\"\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"https://mip-bdcs-vm28.mip.storage.hpecorp.net:10022\"\n",
    "os.environ[\"MLFLOW_TRACKING_INSECURE_TLS\"] = \"true\"\n",
    "os.environ[\"MLFLOW_S3_IGNORE_TLS\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:loaddata"
    ]
   },
   "outputs": [],
   "source": [
    "#Adding package to env\n",
    "conda_env = mlflow.sklearn.get_default_conda_env()\n",
    "conda_env['dependencies'] += ['dill']\n",
    "\n",
    "EXPERIMENT_NAME = \"Loan-Approval-Prediction\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "EXPERIMENT_ID = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id\n",
    "\n",
    "client = Minio(\n",
    "    endpoint=os.getenv(\"MLFLOW_S3_ENDPOINT_URL\").replace(\"https://\",\"\"),\n",
    "    access_key=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    secret_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    secure=True,\n",
    "    http_client = urllib3.PoolManager(cert_reqs='CERT_NONE')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_obj = client.get_object(os.getenv(\"BUCKET\"),\"train.csv\")\n",
    "train_df = pd.read_csv(train_obj)\n",
    "\n",
    "test_obj = client.get_object(os.getenv(\"BUCKET\"),\"test.csv\")\n",
    "test_df = pd.read_csv(test_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pre-processing\n",
    "#Dropping unwanted columns - Loan_ID\n",
    "train_df.drop(['Loan_ID'],axis=1,inplace=True)\n",
    "test_df.drop(['Loan_ID'],axis=1,inplace=True)\n",
    "\n",
    "#Convert target variable to integer\n",
    "train_df['Loan_Status'] = train_df['Loan_Status'].map({'N':0,'Y':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Let's explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Check distribution of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "train_df.Loan_Status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Missing data**\n",
    "\n",
    "Let's see here how much data is missing. We will have to fill the missing features later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "def get_missing_data(df):\n",
    "    total = df.isnull().sum().sort_values(ascending=False)\n",
    "    percent_1 = df.isnull().sum()/df.isnull().count()*100\n",
    "    percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n",
    "    return missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "get_missing_data(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "get_missing_data(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "#Univariate Analysis\n",
    "fig,ax = plt.subplots(2,4,figsize=(16,10))\n",
    "sns.countplot('Loan_Status',data=train_df,ax=ax[0][0])\n",
    "sns.countplot('Gender',data=train_df,ax=ax[0][1])\n",
    "sns.countplot('Married',data=train_df,ax=ax[0][2])\n",
    "sns.countplot('Education',data=train_df,ax=ax[0][3])\n",
    "sns.countplot('Self_Employed',data=train_df,ax=ax[1][0])\n",
    "sns.countplot('Property_Area',data=train_df,ax=ax[1][1])\n",
    "sns.countplot('Credit_History',data=train_df,ax=ax[1][2])\n",
    "sns.countplot('Dependents',data=train_df,ax=ax[1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Explore variable 'Married' with target variable - 'Loan_Status'\n",
    "\n",
    "f,ax=plt.subplots(1,2,figsize=(8,6))\n",
    "train_df['Loan_Status'].value_counts().plot.pie(ax=ax[0],explode=[0,0.1],shadow=True,autopct='%1.1f%%')\n",
    "ax[0].set_title('Loan_Status',fontsize=30)\n",
    "ax[0].set_ylabel('Count')\n",
    "sns.set(font=\"Verdana\")\n",
    "sns.set_style(\"ticks\")\n",
    "sns.countplot('Loan_Status',hue='Married',linewidth=2.5,edgecolor=\".2\",data=train_df,ax=ax[1])\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "#Catplot - 'Married' Vs 'Loan_Status' \n",
    "sns.catplot(x='Married',y='Loan_Status',kind='point',data=train_df)\n",
    "\n",
    "#If you are married, then chances of getting loan approved are more (71.8%) than if you are not married (62.9%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "#Catplot - 'Gender' Vs 'Loan_Status' with hue='Education' and columns = 'Property_Area'\n",
    "sns.catplot(x='Gender',y='Loan_Status',kind='bar',data=train_df,col='Property_Area',hue='Education')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Variable - 'Education' Vs 'Loan_Status'\n",
    "f,ax=plt.subplots(1,2,figsize=(10,8))\n",
    "train_df['Education'].value_counts().plot.pie(ax=ax[0],explode=[0,0.1],shadow=True,autopct='%1.1f%%')\n",
    "ax[0].set_title('Education',fontsize=30)\n",
    "ax[0].set_ylabel('Count')\n",
    "sns.set(font=\"Verdana\")\n",
    "sns.set_style(\"ticks\")\n",
    "sns.countplot('Education',hue='Loan_Status',linewidth=2.5,edgecolor=\".2\",data=train_df,ax=ax[1])\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "#Catplot - 'Education' Vs 'Loan_Status'\n",
    "sns.catplot(x='Education',y='Loan_Status',kind='bar',data=train_df,col='Married',hue='Property_Area')\n",
    "\n",
    "#If you are in urban+Not graduate+not married = Loan approval is less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Variable - 'Property_Area' Vs 'Loan_Status'\n",
    "f,ax=plt.subplots(1,2,figsize=(8,6))\n",
    "train_df['Property_Area'].value_counts().plot.pie(ax=ax[0],explode=[0,0,0.1],shadow=True,autopct='%1.1f%%')\n",
    "ax[0].set_title('Property_Area',fontsize=30)\n",
    "ax[0].set_ylabel('Count')\n",
    "sns.set(font=\"Verdana\")\n",
    "sns.set_style(\"ticks\")\n",
    "sns.countplot('Property_Area',hue='Loan_Status',linewidth=2.5,edgecolor=\".2\",data=train_df,ax=ax[1])\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "#Catplot - 'Property_Area' Vs 'Loan_Status'\n",
    "sns.catplot(x='Property_Area',y='Loan_Status',kind='bar',data=train_df)\n",
    "\n",
    "#If you are in semiurban, then chances of getting loan approved are more (76.8%) than if you are in urban(65.8%) & Rural (61.5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Variable - 'Gender'  Vs 'Loan_Status'\n",
    "f,ax=plt.subplots(1,2,figsize=(10,6))\n",
    "train_df['Gender'].value_counts().plot.pie(ax=ax[0],explode=[0,0.1],shadow=True,autopct='%1.1f%%')\n",
    "ax[0].set_title('Gender',fontsize=30)\n",
    "ax[0].set_ylabel('Count')\n",
    "sns.set(font=\"Verdana\")\n",
    "sns.set_style(\"ticks\")\n",
    "sns.countplot('Gender',hue='Loan_Status',linewidth=2.5,edgecolor=\".2\",data=train_df,ax=ax[1])\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Variable - 'Gender'  Vs 'ApplicantIncome' & hue = Loan_Status, col = Proprty_Area\n",
    "sns.catplot(x='Gender',y='ApplicantIncome',data=train_df,kind='boxen',hue='Loan_Status', col='Property_Area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Variable - 'Gender'  Vs 'CoapplicantIncome'\n",
    "sns.catplot(x='Gender',y='CoapplicantIncome',data=train_df,kind='box')\n",
    "\n",
    "#Mean CoapplicantIncome of male slightly higer than Females."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Variable - 'Gender'  Vs 'CoapplicantIncome' with hue=Loan_Status and col=Property_Area\n",
    "sns.catplot(x='Gender',y='CoapplicantIncome',data=train_df,kind='boxen',hue='Loan_Status', col='Property_Area')\n",
    "\n",
    "#Male have higher co-applicant income than female in all three property areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Variable - 'Dependents' Vs 'Loan_Status'\n",
    "f,ax=plt.subplots(1,2,figsize=(8,6))\n",
    "train_df['Dependents'].value_counts().plot.pie(ax=ax[0],shadow=True,autopct='%1.1f%%')\n",
    "ax[0].set_title('Dependents',fontsize=30)\n",
    "ax[0].set_ylabel('Count')\n",
    "sns.set(font=\"Verdana\")\n",
    "sns.set_style(\"ticks\")\n",
    "sns.countplot('Dependents',hue='Loan_Status',linewidth=2.5,edgecolor=\".2\",data=train_df,ax=ax[1])\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Variable - 'Credit_History' Vs 'Loan_Status'\n",
    "f,ax=plt.subplots(1,2,figsize=(8,6))\n",
    "train_df['Credit_History'].value_counts().plot.pie(ax=ax[0],shadow=True,autopct='%1.1f%%')\n",
    "ax[0].set_title('Credit_History',fontsize=30)\n",
    "ax[0].set_ylabel('Count')\n",
    "sns.set(font=\"Verdana\")\n",
    "sns.set_style(\"ticks\")\n",
    "sns.countplot('Credit_History',hue='Loan_Status',linewidth=2.5,edgecolor=\".2\",data=train_df,ax=ax[1])\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Variable - 'Credit_History' Vs 'Loan_Status'\n",
    "sns.catplot(x='Credit_History',y='Loan_Status',kind='bar',data=train_df)\n",
    "\n",
    "#If credit history is 1 then high chances (79.6%) of getting loan approved than 7.9% for credit history = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Variable - 'Gender' Vs 'LoanAmount'\n",
    "sns.catplot(x='Gender',y='LoanAmount',data=train_df,kind='box')\n",
    "\n",
    "#Mean LoanAmount of male slightly higer than Females."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Variable - 'Self_Employed' Vs 'LoanAmount'\n",
    "sns.catplot(x='Self_Employed',y='LoanAmount',data=train_df,kind='box')\n",
    "\n",
    "#If you are self employed then loan amount is higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Variable - 'Gender' Vs 'LoanAmount', hue='Loan_Status', col='Married'\n",
    "sns.catplot(x='Gender',y='LoanAmount',data=train_df,kind='box',hue='Loan_Status', col='Married')\n",
    "\n",
    "#If you are married then loan amount is slightly higher then non-married"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Variable - 'Loan_Amount_Term' Vs 'Loan_Status'\n",
    "sns.countplot('Loan_Amount_Term',hue='Loan_Status',linewidth=2.5,edgecolor=\".2\",data=train_df)\n",
    "\n",
    "#Maximum customers went for 360 months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DATA PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Imputing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:dataprocessing",
     "prev:loaddata"
    ]
   },
   "outputs": [],
   "source": [
    "#Column - Married. Fill Null values with mode of Married column\n",
    "train_df['Married'].value_counts().index[0]\n",
    "train_df['Married'].fillna(train_df['Married'].value_counts().index[0], inplace=True)\n",
    "test_df['Married'].fillna(test_df['Married'].value_counts().index[0], inplace=True)\n",
    "\n",
    "#Column - Dependents. \n",
    "#If loan status is 1 then, dependent = 2 else dependent=1 for train \n",
    "train_df.loc[(train_df.Dependents.isnull())&(train_df.Loan_Status==1),'Dependents'] = '2'\n",
    "train_df.loc[(train_df.Dependents.isnull()),'Dependents'] = '1'\n",
    "#If Credit_History is 1 then, dependent = 2 else dependent=1 for test\n",
    "test_df.loc[(test_df.Dependents.isnull())&(test_df.Credit_History==1),'Dependents'] = '2'\n",
    "test_df.loc[(test_df.Dependents.isnull()),'Dependents'] = '1'\n",
    "\n",
    "#Column - Credit_History\n",
    "# If loan status is 1 then, Credit_History = 1 else Credit_History=0\n",
    "train_df.loc[(train_df.Credit_History.isnull())&(train_df.Loan_Status==1),'Credit_History'] = 1\n",
    "train_df.loc[(train_df.Credit_History.isnull()),'Credit_History'] = 0\n",
    "# Fill Null values with mode of Credit_History column for test\n",
    "test_df['Credit_History'].fillna(test_df['Credit_History'].value_counts().index[0], inplace=True)\n",
    "#In test data, for the user with income = 2733, it was decided to impute credit history as 0 based upon the Income to loan ratio \n",
    "test_df.loc[(test_df.ApplicantIncome == 2733),'Credit_History']  = 0\n",
    "\n",
    "#Column - Gender\n",
    "# Fill Null values with mode of Gender column\n",
    "train_df['Gender'].fillna(train_df['Gender'].value_counts().index[0], inplace=True)\n",
    "test_df['Gender'].fillna(test_df['Gender'].value_counts().index[0], inplace=True)\n",
    "\n",
    "#Column - Self_Employed\n",
    "# If Credit_History is 1 then, impute Self_Employed = No else Yes\n",
    "train_df.loc[(train_df.Self_Employed.isnull())&(train_df.Credit_History==1),'Self_Employed'] ='No'\n",
    "train_df.loc[(train_df.Self_Employed.isnull()),'Self_Employed'] = 'Yes'\n",
    "# If Credit_History is 1 then, impute Self_Employed = No else Yes for test\n",
    "test_df.loc[(test_df.Self_Employed.isnull())&(test_df.Credit_History==1),'Self_Employed'] ='No'\n",
    "test_df.loc[(test_df.Self_Employed.isnull()),'Self_Employed'] = 'Yes'\n",
    "\n",
    "#Column - Loan_Amount_Term. \n",
    "#Loan_Amount_Term depends upon gender, married, education, self employed and dependent columns. \n",
    "#Hence we will group them by above columns and imput median values. In case if the median is null then we will impute median of the entire Loan_Amount_Term column\n",
    "#get the index of the null columns for Loan_Amount_Term - train\n",
    "index_NaN_Loan_Amount_Term = list(train_df[\"Loan_Amount_Term\"][train_df[\"Loan_Amount_Term\"].isnull()].index)\n",
    "\n",
    "for i in index_NaN_Loan_Amount_Term :\n",
    "    Loan_Amount_Term_med = train_df[\"Loan_Amount_Term\"].median() # find median of entire Loan_Amount_Term column\n",
    "    Loan_Amount_Term_pred = train_df[\"Loan_Amount_Term\"][((train_df['Gender'] == train_df.iloc[i][\"Gender\"])\n",
    "                                                      & (train_df['Married'] == train_df.iloc[i][\"Married\"])\n",
    "                                                      & (train_df['Education'] == train_df.iloc[i][\"Education\"])\n",
    "                                                      & (train_df['Self_Employed'] == train_df.iloc[i][\"Self_Employed\"])\n",
    "                                                      & (train_df['Dependents'] == train_df.iloc[i][\"Dependents\"]))].median()\n",
    "    if not np.isnan(Loan_Amount_Term_pred) :\n",
    "        train_df['Loan_Amount_Term'].iloc[i] = Loan_Amount_Term_pred\n",
    "    else :\n",
    "        train_df['Loan_Amount_Term'].iloc[i] = Loan_Amount_Term_med\n",
    "\n",
    "#Impute Loan term  amount for test\n",
    "index_NaN_Loan_Amount_Term_test = list(test_df[\"Loan_Amount_Term\"][test_df[\"Loan_Amount_Term\"].isnull()].index)\n",
    "\n",
    "for i in index_NaN_Loan_Amount_Term_test :\n",
    "    Loan_Amount_Term_med = test_df[\"Loan_Amount_Term\"].median() \n",
    "    Loan_Amount_Term_pred = test_df[\"Loan_Amount_Term\"][(( test_df['Gender'] == test_df.iloc[i][\"Gender\"])\n",
    "                                                         & (test_df['Married'] == test_df.iloc[i][\"Married\"])\n",
    "                                                         & (test_df['Education'] == test_df.iloc[i][\"Education\"])\n",
    "                                                         & (test_df['Self_Employed'] == test_df.iloc[i][\"Self_Employed\"])\n",
    "                                                         & (test_df['Dependents'] == test_df.iloc[i][\"Dependents\"]))].median()\n",
    "    if not np.isnan(Loan_Amount_Term_pred) :\n",
    "        test_df['Loan_Amount_Term'].iloc[i] = Loan_Amount_Term_pred\n",
    "    else :\n",
    "        test_df['Loan_Amount_Term'].iloc[i] = Loan_Amount_Term_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# Variable - LoanAmount - Identify on which other columns LoanAmount depends\n",
    "# Explore LoanAmount vs categorical variables\n",
    "g = sns.factorplot(y=\"LoanAmount\",x=\"Credit_History\",data=train_df,kind=\"box\",hue=\"Gender\")\n",
    "g = sns.factorplot(y=\"LoanAmount\",x=\"Property_Area\",data=train_df,kind=\"box\", hue=\"Dependents\")\n",
    "g = sns.factorplot(y=\"LoanAmount\",x=\"Gender\", data=train_df,kind=\"box\",hue=\"Married\")\n",
    "g = sns.factorplot(y=\"LoanAmount\",x=\"Married\", data=train_df,kind=\"box\")\n",
    "g = sns.factorplot(y=\"LoanAmount\",x=\"Education\", data=train_df,kind=\"box\")\n",
    "g = sns.factorplot(y=\"LoanAmount\",x=\"Self_Employed\", data=train_df,kind=\"box\")\n",
    "g = sns.factorplot(y=\"LoanAmount\",x=\"Dependents\", data=train_df,kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Column - LoanAmount. LoanAmount depends upon Property_Area, gender, married, education, self employed and dependent columns. \n",
    "# Hence we will group them by above columns and imput median values. In case if the median is null then we will impute median of the entire LoanAmount column.\n",
    "\n",
    "#Impute LoanAmount for train\n",
    "index_NaN_LoanAmount = list(train_df[\"LoanAmount\"][train_df[\"LoanAmount\"].isnull()].index)\n",
    "\n",
    "for i in index_NaN_LoanAmount :\n",
    "    LoanAmount_med = train_df[\"LoanAmount\"].median() # find median of entire LoanAmount column\n",
    "    LoanAmount_pred = train_df[\"LoanAmount\"][((train_df['Property_Area'] == train_df.iloc[i][\"Property_Area\"])\n",
    "                                          & (train_df['Gender'] == train_df.iloc[i][\"Gender\"])\n",
    "                                          & (train_df['Married'] == train_df.iloc[i][\"Married\"])\n",
    "                                          & (train_df['Education'] == train_df.iloc[i][\"Education\"])\n",
    "                                          & (train_df['Self_Employed'] == train_df.iloc[i][\"Self_Employed\"])\n",
    "                                          & (train_df['Dependents'] == train_df.iloc[i][\"Dependents\"]))].median()\n",
    "    if not np.isnan(LoanAmount_pred) :\n",
    "        train_df['LoanAmount'].iloc[i] = LoanAmount_pred\n",
    "    else :\n",
    "        train_df['LoanAmount'].iloc[i] = LoanAmount_med\n",
    "        \n",
    "#Impute Loan amount for test\n",
    "index_NaN_LoanAmount_test = list(test_df[\"LoanAmount\"][test_df[\"LoanAmount\"].isnull()].index)\n",
    "\n",
    "for i in index_NaN_LoanAmount_test :\n",
    "    LoanAmount_med = test_df[\"LoanAmount\"].median()\n",
    "    LoanAmount_pred = test_df[\"LoanAmount\"][((test_df['Property_Area'] == test_df.iloc[i][\"Property_Area\"])\n",
    "                                          & (test_df['Gender'] == test_df.iloc[i][\"Gender\"])\n",
    "                                          & (test_df['Married'] == test_df.iloc[i][\"Married\"])\n",
    "                                          & (test_df['Education'] == test_df.iloc[i][\"Education\"])\n",
    "                                          & (test_df['Self_Employed'] == test_df.iloc[i][\"Self_Employed\"])\n",
    "                                          & (test_df['Dependents'] == test_df.iloc[i][\"Dependents\"]))].median()\n",
    "    if not np.isnan(LoanAmount_pred) :\n",
    "        test_df['LoanAmount'].iloc[i] = LoanAmount_pred\n",
    "    else :\n",
    "        test_df['LoanAmount'].iloc[i] = LoanAmount_med"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Check for any missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "get_missing_data(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "get_missing_data(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Label encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df[\"Dependents\"] = train_df[\"Dependents\"].map({\"0\": 0, \"1\": 1,\"2\": 2, \"3+\": 3})\n",
    "train_df[\"Property_Area\"] = train_df[\"Property_Area\"].map({\"Rural\":0, \"Semiurban\":1, \"Urban\": 2,})\n",
    "\n",
    "test_df[\"Dependents\"] = test_df[\"Dependents\"].map({\"0\": 0, \"1\": 1,\"2\": 2, \"3+\": 3})\n",
    "test_df[\"Property_Area\"] = test_df[\"Property_Area\"].map({\"Rural\":0, \"Semiurban\":1, \"Urban\": 2,})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Since LoanAmount is in thousands, lets multiply LoanAmount column with 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['LoanAmount'] = train_df['LoanAmount'] * 1000\n",
    "test_df['LoanAmount'] = test_df['LoanAmount'] * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "#Final Pair plot\n",
    "sns.heatmap(train_df.corr(),annot=True,cmap='RdYlGn',linewidths=0.2,annot_kws={'size':12})\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(10,6)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Combine train and test datasets\n",
    "combine_set=pd.concat([train_df,test_df], ignore_index=True)\n",
    "combine_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:featureengineering",
     "prev:dataprocessing"
    ]
   },
   "outputs": [],
   "source": [
    "#Add new features - Total income\n",
    "combine_set['Total_Income'] = combine_set['ApplicantIncome'] + combine_set['CoapplicantIncome']\n",
    "\n",
    "#Dropping unwanted columns - 'ApplicantIncome','CoapplicantIncome'\n",
    "combine_set.drop(['ApplicantIncome','CoapplicantIncome'],axis=1,inplace=True)\n",
    "\n",
    "# Categorical variables wise sum of numerical columns\n",
    "combine_set['Credit_History_Income_Sum']=combine_set.groupby(['Credit_History'])['Total_Income'].transform('sum')\n",
    "combine_set['Dependents_LoanAmount_Sum']=combine_set.groupby(['Dependents'])['LoanAmount'].transform('sum')\n",
    "\n",
    "#EMI\n",
    "#Lets assume that interest rate=10.0 # hence r = ((10/12)/100) = 0.00833\n",
    "\n",
    "r = 0.00833\n",
    "combine_set['EMI']=combine_set.apply(lambda x: (x['LoanAmount']*r*((1+r)**x['Loan_Amount_Term']))/((1+r)**((x['Loan_Amount_Term'])-1)),axis=1)\n",
    "\n",
    "# Categorical variables wise mean of EMI\n",
    "combine_set['Dependents_EMI_mean']=combine_set.groupby(['Dependents'])['EMI'].transform('mean')\n",
    "\n",
    "# LoanAmount_per_Total_Income\n",
    "combine_set['LoanAmount_per_Total_Income']=combine_set['LoanAmount']/combine_set['Total_Income']\n",
    "\n",
    "# Loan_Amount_Term_per_Total_Income\n",
    "combine_set['Loan_Amount_Term_per_Total_Income']=combine_set['Loan_Amount_Term']/combine_set['Total_Income']\n",
    "\n",
    "# EMI_per_Loan_Amount_Term\n",
    "combine_set['EMI_per_Loan_Amount_Term']=combine_set['EMI']/combine_set['Loan_Amount_Term']\n",
    "\n",
    "# EMI_per_LoanAmount\n",
    "combine_set['EMI_per_LoanAmount']=combine_set['EMI']/combine_set['LoanAmount']\n",
    "\n",
    "# Categorical variables wise mean of LoanAmount_per_Total_Income\n",
    "combine_set['Property_Area_LoanAmount_per_Total_Income_mean']=combine_set.groupby(['Property_Area'])['LoanAmount_per_Total_Income'].transform('mean')\n",
    "\n",
    "\n",
    "################################# Bin formation ###############################################\n",
    "\n",
    "Loan_Amount_Term_discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "combine_set['Loan_Amount_Term_Bins'] = Loan_Amount_Term_discretizer.fit_transform(combine_set['Loan_Amount_Term'].values.reshape(-1,1)).astype(float)\n",
    "\n",
    "Total_Income_discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "combine_set['Total_Income_Bins'] = Total_Income_discretizer.fit_transform(combine_set['Total_Income'].values.reshape(-1,1)).astype(float)\n",
    "\n",
    "LoanAmount_per_Total_Income_discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "combine_set['LoanAmount_per_Total_Income_Bins'] = LoanAmount_per_Total_Income_discretizer.fit_transform(combine_set['LoanAmount_per_Total_Income'].values.reshape(-1,1)).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Drop unwanted columns\n",
    "combine_set=combine_set.drop(['EMI'],axis=1)\n",
    "combine_set=combine_set.drop(['Total_Income'],axis=1)\n",
    "combine_set=combine_set.drop(['LoanAmount_per_Total_Income'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "combine_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:",
     "prev:featureengineering"
    ]
   },
   "outputs": [],
   "source": [
    "#Segregate train and test\n",
    "train_df=combine_set[combine_set['Loan_Status'].isnull()==False]\n",
    "test_df=combine_set[combine_set['Loan_Status'].isnull()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Label encode categorical variables using get_dummies()\n",
    "train_df = pd.get_dummies(train_df, drop_first = True)\n",
    "test_df = pd.get_dummies(test_df, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:"
    ]
   },
   "outputs": [],
   "source": [
    "#Drop target variable from X and copy to y\n",
    "X = train_df.drop(['Loan_Status'],axis=1)\n",
    "y = train_df['Loan_Status']\n",
    "\n",
    "#Drop target column (which is blank) from test dataset\n",
    "X_main_test=test_df.drop(['Loan_Status'],axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "functions"
    ]
   },
   "outputs": [],
   "source": [
    "def eval_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    return rmse, mae, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Custom class for prediction and probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SklearnModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def predict(self, context, model_input):\n",
    "        import logging\n",
    "        logger = logging.getLogger()\n",
    "        probability = self.model.predict_proba(model_input)[:,1]\n",
    "        status = self.model.predict(model_input)\n",
    "        \n",
    "        output = [ {\"probability\": probability[i], \"Loan_Status\": status[i] } for i in range(0, len(probability))]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:randomforest",
     "prev:featureengineering"
    ]
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='Random Forest Classifier', experiment_id=EXPERIMENT_ID):\n",
    "    random_forest = RandomForestClassifier(n_estimators=100, max_depth=3, min_samples_leaf = 10)\n",
    "    random_forest.fit(X_train, y_train)\n",
    "\n",
    "    predicted_qualities = random_forest.predict(X_test)\n",
    "\n",
    "    (rmse, mae, r2) = eval_metrics(y_test, predicted_qualities)\n",
    "    accuracy = round(random_forest.score(X, y) * 100, 2)\n",
    "    print(\"  RMSE: %s\" % rmse)\n",
    "    print(\"  MAE: %s\" % mae)\n",
    "    print(\"  R2: %s\" % r2)\n",
    "    print(\"  Accuracy: %s\" % accuracy)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    wrappedModel = SklearnModelWrapper(random_forest)\n",
    "    mlflow.pyfunc.log_model(\"model\", python_model=wrappedModel, conda_env=conda_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:logisticregression",
     "prev:featureengineering"
    ]
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='Logistic Regression', experiment_id=EXPERIMENT_ID):\n",
    "    logreg = LogisticRegression(solver='lbfgs', max_iter=110)\n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    predicted_qualities = logreg.predict(X_test)\n",
    "\n",
    "    (rmse, mae, r2) = eval_metrics(y_test, predicted_qualities)\n",
    "    accuracy = round(logreg.score(X, y) * 100, 2)\n",
    "    print(\"  RMSE: %s\" % rmse)\n",
    "    print(\"  MAE: %s\" % mae)\n",
    "    print(\"  R2: %s\" % r2)\n",
    "    print(\"  Accuracy: %s\" % accuracy)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    wrappedModel = SklearnModelWrapper(logreg)\n",
    "    mlflow.pyfunc.log_model(\"model\", python_model=wrappedModel, conda_env=conda_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:naivebayes",
     "prev:featureengineering"
    ]
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='Gaussian Naive Bayes', experiment_id=EXPERIMENT_ID):\n",
    "    gaussian = GaussianNB()\n",
    "    gaussian.fit(X_train, y_train)\n",
    "\n",
    "    predicted_qualities = gaussian.predict(X_test)\n",
    "\n",
    "    (rmse, mae, r2) = eval_metrics(y_test, predicted_qualities)\n",
    "    accuracy = round(gaussian.score(X, y) * 100, 2)\n",
    "    print(\"  RMSE: %s\" % rmse)\n",
    "    print(\"  MAE: %s\" % mae)\n",
    "    print(\"  R2: %s\" % r2)\n",
    "    print(\"  Accuracy: %s\" % accuracy)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    wrappedModel = SklearnModelWrapper(gaussian)\n",
    "    mlflow.pyfunc.log_model(\"model\", python_model=wrappedModel, conda_env=conda_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:svm",
     "prev:featureengineering"
    ]
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='SVM', experiment_id=EXPERIMENT_ID):\n",
    "    linear_svc = SVC(gamma='auto',probability=True)\n",
    "    linear_svc.fit(X_train, y_train)\n",
    "\n",
    "    predicted_qualities = linear_svc.predict(X_test)\n",
    "\n",
    "    (rmse, mae, r2) = eval_metrics(y_test, predicted_qualities)\n",
    "    accuracy = round(linear_svc.score(X, y) * 100, 2)\n",
    "    print(\"  RMSE: %s\" % rmse)\n",
    "    print(\"  MAE: %s\" % mae)\n",
    "    print(\"  R2: %s\" % r2)\n",
    "    print(\"  Accuracy: %s\" % accuracy)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    wrappedModel = SklearnModelWrapper(linear_svc)\n",
    "    mlflow.pyfunc.log_model(\"model\", python_model=wrappedModel, conda_env=conda_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:decisiontree",
     "prev:featureengineering"
    ]
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='Decision Tree', experiment_id=EXPERIMENT_ID):\n",
    "    decision_tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=5)\n",
    "    decision_tree.fit(X_train, y_train)\n",
    "\n",
    "    predicted_qualities = decision_tree.predict(X_test)\n",
    "\n",
    "    (rmse, mae, r2) = eval_metrics(y_test, predicted_qualities)\n",
    "    accuracy = round(decision_tree.score(X, y) * 100, 2)\n",
    "    print(\"  RMSE: %s\" % rmse)\n",
    "    print(\"  MAE: %s\" % mae)\n",
    "    print(\"  R2: %s\" % r2)\n",
    "    print(\"  Accuracy: %s\" % accuracy)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    wrappedModel = SklearnModelWrapper(decision_tree)\n",
    "    mlflow.pyfunc.log_model(\"model\", python_model=wrappedModel, conda_env=conda_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:lda",
     "prev:featureengineering"
    ]
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='Linear Discriminant Analysis', experiment_id=EXPERIMENT_ID):\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(X_train, y_train)\n",
    "\n",
    "    predicted_qualities = lda.predict(X_test)\n",
    "\n",
    "    (rmse, mae, r2) = eval_metrics(y_test, predicted_qualities)\n",
    "    accuracy = round(lda.score(X, y) * 100, 2)\n",
    "    print(\"  RMSE: %s\" % rmse)\n",
    "    print(\"  MAE: %s\" % mae)\n",
    "    print(\"  R2: %s\" % r2)\n",
    "    print(\"  Accuracy: %s\" % accuracy)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    wrappedModel = SklearnModelWrapper(lda)\n",
    "    mlflow.pyfunc.log_model(\"model\", python_model=wrappedModel, conda_env=conda_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:knn",
     "prev:featureengineering"
    ]
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='KNN', experiment_id=EXPERIMENT_ID):\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    predicted_qualities = knn.predict(X_test)\n",
    "\n",
    "    (rmse, mae, r2) = eval_metrics(y_test, predicted_qualities)\n",
    "    accuracy = round(knn.score(X, y) * 100, 2)\n",
    "    print(\"  RMSE: %s\" % rmse)\n",
    "    print(\"  MAE: %s\" % mae)\n",
    "    print(\"  R2: %s\" % r2)\n",
    "    print(\"  Accuracy: %s\" % accuracy)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    wrappedModel = SklearnModelWrapper(knn)\n",
    "    mlflow.pyfunc.log_model(\"model\", python_model=wrappedModel, conda_env=conda_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:adaboost",
     "prev:featureengineering"
    ]
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='AdaBoost', experiment_id=EXPERIMENT_ID):\n",
    "    adaboost = AdaBoostClassifier()\n",
    "    adaboost.fit(X_train, y_train)\n",
    "\n",
    "    predicted_qualities = adaboost.predict(X_test)\n",
    "\n",
    "    (rmse, mae, r2) = eval_metrics(y_test, predicted_qualities)\n",
    "    accuracy = round(adaboost.score(X, y) * 100, 2)\n",
    "    print(\"  RMSE: %s\" % rmse)\n",
    "    print(\"  MAE: %s\" % mae)\n",
    "    print(\"  R2: %s\" % r2)\n",
    "    print(\"  Accuracy: %s\" % accuracy)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    wrappedModel = SklearnModelWrapper(adaboost)\n",
    "    mlflow.pyfunc.log_model(\"model\", python_model=wrappedModel, conda_env=conda_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:results",
     "prev:naivebayes",
     "prev:svm",
     "prev:decisiontree",
     "prev:adaboost",
     "prev:lda",
     "prev:knn",
     "prev:logisticregression",
     "prev:randomforest"
    ]
   },
   "outputs": [],
   "source": [
    "best_run_df = mlflow.search_runs(experiment_ids=[EXPERIMENT_ID], order_by=['metrics.accuracy DESC'], max_results=1)\n",
    "best_run = mlflow.get_run(best_run_df.at[0, 'run_id'])\n",
    "best_model_uri = f\"{best_run.info.artifact_uri}/model\"\n",
    "best_model = pyfunc.load_model(best_model_uri)\n",
    "\n",
    "print(\"Best run info:\")\n",
    "print(f\"Run id: {best_run.info.run_id}\")\n",
    "print(\"Run Accuracy:  = {:.4f}\\n\".format(best_run.data.metrics['accuracy']))\n",
    "print(f\"Run model URI: {best_model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "devsds/kubeflow-kale:latest",
   "experiment": {
    "id": "new",
    "name": "loan-approval-prediction"
   },
   "experiment_name": "loan-approval-prediction",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "Pipeline for trainig mutliple sklarn classification model and looging metrics to MLFlow tracking server.",
   "pipeline_name": "pipeline-loan-approval-prediction",
   "snapshot_volumes": false,
   "steps_defaults": [],
   "volume_access_mode": "rwm",
   "volumes": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
